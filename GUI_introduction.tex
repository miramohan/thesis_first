\chapter{INTRODUCTION}
%\section{Background}
%\section{Purpose/Motivation}
%\section{Approach}
%\section{Main Contributions}
%\section{Organization of thesis}

With the rapid growth of deep sub micron technology, there has been an aggressive shrinking in physical dimension of silicon structure that can be realized on a single chip. This advancement has enabled the transition of multi-million gate designs from large printed circuit boards to SoC (System on Chip). SoC design has the advantages of smaller size, low power consumption, reliability, performance improvement and low cost per gate. Another major high point of SoC from design point of view is that SoC allows use of predesigned blocks called semiconductor intellectual property (IP). These hardware IP blocks can be mix-and-matched, thereby providing design reuse in SoC and thereby reducing time-to market. 


 Over the past few years major competition in semiconductor industry is to develop more complex SoCs with greater functionality and diversity but in very less time. The main challenge to this is verification. Integration between various components, combined complexity of multiple sub-systems, software-hardware co-verification, conflicts in accessing shared resource, arbitration problems and dead-locks, priority conflicts in exception handling etc makes SoC verification very hard. It is said that verification consumes more than 60 percent of design effort. This can be easily explained as there is no single design tool that can completely verify a SoC on it's on. Instead a complex sequence of tools and techniques, including simulation, directed and random verification and formal verification are used to verify a SoC. Even then a complete cent percent verification coverage is never really achieved because of time-to-market constraints.


\section{VERIFICATION METHODS}
Two widely adopted verification methods are simulation based and formal verification method. In simulation based verification the design engineer develops a set of tests, based on design specification. Design correctness is then established through simulated results. Formal verification is a mathematical proof method of ensuring that a design's implementation matches its specification. The most prominent distinction between simulation-based verification and formal verification is that the former requires input vectors and the latter does not. In simulation based verification the idea is to first generate input vectors and then derive the reference output where as in formal verification process it is the reverse approach. Here first the desired output behavior is stated and then lets the formal checker prove. Here user is not concerned with input stimulus at all. ive memory utilization and long run time before a verification decision is reached.  An

At SoC level the designs are huge, typically beyond the capacities of automatic formal tools. Tools have extensive memory utilization and long run time before a verification decision is reached.  And when memory capacity is exceeded, tools often shed little light on what went wrong, or give little guidance to fix the problem. As a result, formal verification software, is applicable only to circuits of moderate size, such as blocks or modules. And hence often a simulation based method is also adopted for verification of top level design along with the aid of formal techniques. 

\section{SoC VERIFICATION IN AMD}
In AMD, most SoCs are built around one or more x86 cores (multi processor) and verification is done using the simulation based verification method where the design model is simulated using random or handwritten test programs. Reference models are simulated in parallel to check that there results matches those of the design.  Comparison between the design architecture state and the reference model states are done after each instruction retire. Difference detected in states is considered as "{\it mismatches}". Also memory comparisons are done at the end of simulation and any discrepancies are reported as memory mismatches.  The simulator writes entries for each event it processes into processor execution log file. 

On the event of a simulation failure, these log entries are helpful in understanding and tracing the cause associated with the failure. Logs contain in-depth details pertaining to processor execution. 

Tracing a typical failure is a manual process and is time consuming since relevant informations are buried under a wealth of information, and co-related information are spread across in different files. This will greatly increase debug efforts and verification time. If we can provide an interface were these informations are collected and represented in a co-related manner, a significant improvement in the debug time requirement can be achieved. The proposed interactive interface provides graphical data representations and navigation helping in faster tracing through the log details. Informations are co-related and sorted making debugging a lot more easier than manually checking through the files.

 


\section{ORGANIZATION OF THE THESIS}
The organization of this project report is as follows:\\
\noindent 
{\bf Chapter}~\ref{chap:amd64}-{\it AMD64 Architecture Overview} give brief introduction to AMD64 architecture.\\
{\bf Chapter}~\ref{chap:verification.tex}-{\it Verification Environment} discusses verification environment details.\\
{\bf Chapter}~\ref{chap:GUI_impl.tex}-{\it Interface Implementation} gives the detailed implementation of GUI.\\
{\bf Chapter}~\ref{chap:GUI_features.tex}-{\it GUI features} all the GUI features are detailed in this chapter.\\
{\bf Chapter}~\ref{chap:GUI_results.tex}-{\it Results: Graphic User Interface} shows different GUI windows and their features.\\
%{\bf Chapter}~\ref{chap:conclusion} discusses the various 
%conclusion drawn from the results and the scope for future work.

