\chapter{VERIFICATION ENVIRONMENT}
\label{chap:verification.tex}

\section {FUNCTIONAL VERIFICATION}

In SoC design methodology, the first step is to define the specifications. Once enough system specification is available, design phase starts. The behavioral modeling of the design is done using hardware description languages like VHDL or Verilog HDL and in this stage the design is said to be Register Transfer Level (RTL)\nomenclature{RTL}{Register Transfer Level}. Such design could be partitioned to aid reusability, concurrent development and tool effectiveness. In general, reusable components of designs are packaged into components called Intellectual Property (IP). The RTL description of design is verified against functional specifications. System level verifications are done to verify the RTL description against the intended functionality among other requirements such as timing, power and gate-count. 



Functional verification validates that the design meets its requirements. Test cases are created based on specifications. Various aspects of data and control flow are verified by passing information between external environments, communication with I/O devices,  software interactions etc. 
 
\section {VERIFICATION}

Most SoC verification concentrates on verifying the processor cores and their interactions with SoC level IPs. At this level of abstraction, verification is concentrated on interaction between IPs and on design features those haven't got verified as a sub-system.  Test conditions are written in x86 assembly or in some cases written in high level languages like {\it C++}. The intention of each test is to verify specific functionality of the design and ensure its validity. Test plans must be in sync with specifications and are required to be updated with specification changes. Test methodology should also be adaptable to accomodate such changes or to deal with all possible corner cases and boundary conditions.

Tests are developed, so that they stimulate the design in a specific manner and compare outcome against expected outcome to assert accuracy of design behaviour. Ideally design should be verified against all possible scenarios that could arise and once it passes all tests, it can be considered as completely verified. In case when a particular test fails, the verification engineer needs to find out the cause of failure with his understanding of design or verification aspect that led to the particular failure. This process of root causing a failure is called as a ``{\it debug}'' in verification. Once root-caused, the engineer then has to suggest appropriate changes to either design, verification or documentation to keep them in unison. 

There are many possible issues that can lead to a test failure. Each test defines conditions for pass and fail. A fail or pass is basically the outcome of a test run. There can be manly different causes for failures, with majority of them being:

\begin{description}
	\item[Self check fails:] In a self check failure, the program code running on the simulated processor is able to identify and report a failure. The program tests are written such that they evaluate the results, compare it with desired value and finally report fail or pass. 
	\item[Assertion/Checker fails:] Assertion or checker fails are very common kind of failures and occur when a testbench component reports an unexpected behaviour of the design. It could also be a false failure. In general, most checkers or assertions monitor particular design states during the simulation and report failure whenever monitored value deviates from the expected value. 
\end{description}

In general, a self check fail is caused when program execution deviates from the intended execution flow that was determined for the program under test. Hence the debug of a self check fail would require knowledge of the program under test and its intended execution flow. Without these knowledge, it would be a challenge to debug such fails. This project is aimed to improve debug of such self check fails.

%\subsection {DEVELOP TESTS}
%
%Tests for verifying all the features of the RTL are written in x86 assembly or in high level languages. These test plans are in sync with the specifications of RTL and are to be updated with new specification changes. Test cases should be efficient enough to deal with all possible []: 
%\begin{itemize}
%
%\item Corner cases
%\item Boundary conditions
%\item Design discontinuities
%\item Error conditions
%\item Exception/Interrupt handling
%
%\end {itemize}

\subsection {SELF CHECK FAILURE}

Processor tests are C/assembly program written to assert that the processor under test is indeed functioning as expected under that specific setup. These processor tests are designed such a way that they are capable of deciding if the processor execution results were successful. These tests are normally hand written by the verification engineer rather than randomly generated.  Hence such tests can string together specific stimulus of interest and determine pass or fail status on its own without relying on other verification components. Such tests are called as ``self~tests''.


%\figurename{} 
\begin{figure}[h]
\centering
\includegraphics[scale=0.65]{./figures/selftest.ps}
\caption{Self~Test} 
\label{fig:selftest.ps}
\end{figure}

\figurename {\ref{fig:selftest.ps}} details the flow of a self~test. Self~tests are a collection of x86 assembly language program. Tests are compiled by a script which calls an assembler followed by a linker. The final output being a linked binary image can be loaded and executed by the RTL model of DUT\nomenclature{DUT}{Design Under Test}. Stimulus is generated by the test and is applied to the DUT. Comparisons are done by the test itself after which pass/fail result is generated.



\section{DEBUGGING A SELF CHECK FAILURE}

Self~tests report the occurrence of test case failure. Once this is available, next step is to analyze the reason for failure.  For this, a traceback from the point of failure to the point of error is required. 

A failure message would indicate which check failed that the result is deviating from the desired value. This desired value can be understood from analysing the asm test code. But to understand at which point during execution the design deviated from desired course, detailed information regarding execution flow as well as a reference flow which has the ideal values and status are required. In general, a reference flow could be established by the engineer after understanding and interpreting the test in its completeness.
 
To aid execution flow, RTL is simulated along with an instruction level reference emulator. The reference emulator is a software model which imitates the design functionally and executes same instructions in parallel with the RTL. This parallel simulation is also called as model cosimulation and produces a log of processor activity.


\subsection {MODEL COSIMULATION}
The instruction level reference emulator (ILE) \nomenclature{ILE}{Instruction Level Emulator} is an x86/x86-64 compatible model, generally written in software languages such as {\it C++}. It models the processor in great detail including registers, caches and modes of operation. The test provides stimulus to both the RTL and reference models. An interface between RTL and reference model compares the states after each instruction retire and reports any mismatches, if any. 

Following sections describes features and functions of emulator and interface. 



%\figurename{} 
\begin{figure}[h]
\centering
\includegraphics[scale=0.65]{./figures/interface.ps}
\caption{RTL-Reference Model Cosimulation} 
\label{fig:interface.ps}
\end{figure}



%\subsection {COMPARISON WITH REFERENCE MODULE}

%The RTL is simulated along with an instruction level reference emulator. This instruction level emulator is an x86/x86-64 programming model. It mainly models the register states and memory features and act as the ideal reference point to compare with. An interface between RTL and reference model compares the states after each instruction retire and report any mismatch. The following section details the features and functions of emulator and interface.

\subsubsection {INSTRUCTION LEVEL EMULATOR}
The x86 ILE starts emulation after initializing the contents of its memory with linked binary image of the test. The ILE emulates the fetch/decode/execute algorithm of a scalar processor, producing an output log known as {\it processor execution log}. Each log entry describes the instruction, its results and any side effects it had on the processor state. The emulator models debug features, exceptions and interrupts as well as processor specific features. Supported processor states include x86 general purpose registers, flag registers, control registers, media registers, model specific registers as well as memory and I/O spaces. The emulator runs multithreaded code to simulate multiprocessor and multicore processor systems.\\ 
The emulator runs in step with the RTL. Whenever an instruction or exception is retired in RTL that thread within the emulator is stepped-up and the processor states in the RTL and the emulator are compared. Difference detected in processor states are considered as a {\it mismatch}; difference in memory locations are also considered as {\it memory mismatches} and upon any {\it mismatch}, the cosimulation is terminated. At the end of RTL simulation when all threads stop executing, the memory states of RTL and the emulator are compared and any discrepancies are reported as memory mismatches.

\subsubsection {INTERFACE}
The interface connects RTL and reference model, and helps to keep the ILE in step with RTL signal. It steps the ILE when an instruction in RTL simulation has retired and then compares the results of execution between RTL and ILE. If the reference model is unable to model anything that is present in the RTL, the interface also re-synchronizes the ILE with the state obtained after execution of the RTL instruction.
Main functions of this interface includes:
\begin{description}
	\item[Initialization:] Initialize memory model, attach with RTL and ILE, and initialize some specific RTL signals.
	\item[Increment:] Based on number of instruction retired per cycle, the interface informs ILE how many instructions to step.
	\item[Interrupt Handling:] Interface informs ILE about pending interrupts.
	\item[Comparing:] Compares RTL and reference model registers; integer, FP, control and status word. Also reports any mismatches.
	\item[Interfacing with the memory model:] Tell memory model what operations are seen by the RTL. 
\end{description}

\label{verif:exelog}

During the course of emulation, the ILE generates log files containing processor's program execution details. These are called {\it processor execution logs}. These log files contains cycle by cycle information regarding register states, memory values, flags, threads etc. These log entries help debug in the event of failure.
%The  generated by ILE, during the course of emulation, contains cycle by cycle information regarding register states, memory values, flags, threads etc. All the comparisons and debugging requires these information.

On the event of a failure, emulation terminates and and debugging is required. In order to root-cause the failure, verification engineer needs to trace through this {\it processor execution log}. Mismatches with reference model values provide information regarding the cause of failure.

Tracing through the log files is done manually. Data obtained from {\it processor execution log} and the assembled test files needs to be analyzed and correlated for debugging the failure. This is a very tedious effort consuming a lot of verification time. This is because of two main reasons:

\begin{enumerate}
	\item Relevant/required information is buried under a wealth of information.
	\item Correlated information is spread across in different files.
\end{enumerate}

As the design itself is very complex, these reasons makes manual tracing too time consuming. This stretches the verification time and ultimately time-to-market.   


